{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8efbf227",
      "metadata": {
        "id": "8efbf227"
      },
      "source": [
        "# Logistic Regression assignment\n",
        "\n",
        "\n",
        "This is **Code help Notebook** for the Logistic Regression assignment.\n",
        "\n",
        "The goal of this assignment is to introduce you to the idea of a **Maximum Entropy Language Model**,\n",
        "that is, a Logistic Regression (LR) model that tries to predict the next word on the basis of features of the word's linguistic context.  We will look at two versions of the model.  One is a simple bigram model formulated as an LR model (the only context feature is the word immediately preceding the word we are predicting,\n",
        "(which we will call the **target**).  The other is a bigram model augmented with **trigger word features**, words that are known triggers for other words, which may be found arbitrarily far from the target.  The idea of building an LR language model is due to [Rosenfeld (1994)](https://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/roni/papers/me-csl-revised.pdf) and we follow his method for finding trigger words.\n",
        "\n",
        "The code in the section entitled **Preparing the filtered corpus** all needs to run in order for\n",
        "this notebook to work.\n",
        "\n",
        "The code in the section entitled  **Finding triggers  using Mutual Information (Rosenfeld 1994)**\n",
        "does not need to run (it takes a while).  You can circumvent it by using the value assigned to\n",
        "`triggers` at the end of that section. The correct value  for `triggers`(a set of 150 words) is spelled out at the end of that section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ee9cae7",
      "metadata": {
        "id": "1ee9cae7"
      },
      "source": [
        "## Preparing the filtered corpus\n",
        "\n",
        "To restrict the number of parameters in our model and yet demonstarte the ideas on reealistic data, we are going to build an LR language model on a **filtered** data set.  We will model onl noun co-occurrences and we will limit our model to nouns occurring over 100 times in the Brown corpus, That is, gioven that Brpwn is about 1.2 million words,\n",
        "we will look at nounds with a relative fdrequency greater than about:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67b844fc",
      "metadata": {
        "id": "67b844fc"
      },
      "source": [
        "This gives us a vocabulary of between 600 and 700 words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae0b9662",
      "metadata": {
        "id": "ae0b9662"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.cluster import contingency_matrix\n",
        "from sklearn.metrics import mutual_info_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26a42da5",
      "metadata": {
        "id": "26a42da5"
      },
      "source": [
        "Corpus prep,  Vocab filter (freq threshold and noun)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc053972",
      "metadata": {
        "id": "fc053972"
      },
      "source": [
        "$$\n",
        "\\begin{array}{rcrrr}\n",
        "k & pos & \\# sents & V & N\\\\\n",
        "\\hline\n",
        "200 & \\text{N} & 28,468  & 258 & 21,709\\\\\n",
        "\\mathbf{100} &  \\text{N}  & \\mathbf{19,243} & \\mathbf{615} & \\mathbf{55, 792} \\\\\n",
        " 1 &        & 57,340  & 49,815 & 1,116,192\\\\\n",
        "\\end{array}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "1d0867e5",
      "metadata": {
        "id": "1d0867e5",
        "outputId": "26602aca-1ce9-4116-ef3c-d7e6d9a936fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1161192\n",
            "Filtered info:  Sents 19243 Vocab 615 N 55792\n",
            "Base info:  Sents 57013 Vocab 49815 N 1160865\n"
          ]
        }
      ],
      "source": [
        "from nltk import FreqDist\n",
        "from nltk.corpus import brown\n",
        "\n",
        "def relevant(w, tag, fd=None, k=100, pos_chars=None):\n",
        "    \"\"\"\n",
        "    pos_char is a usually 'N' or 'V' or 'NV' or to select the nominal or verbal pos sets of Brown\n",
        "    \"\"\"\n",
        "    if pos_chars is not None and not tag[0] in pos_chars:\n",
        "        return False\n",
        "    if fd is not None:\n",
        "        return fd[w] > k\n",
        "    else:\n",
        "        return True\n",
        "\n",
        "\n",
        "bw = [w.lower() for (w,t) in brown.tagged_words()]\n",
        "print(len(bw))\n",
        "fd = FreqDist(bw)\n",
        "tagged_sents = brown.tagged_sents()\n",
        "\n",
        "###############   Filtered vocab #####################################################################\n",
        "\n",
        "#k=200,pos_chars=\"N\"\n",
        "#k=100, pos_chars = \"N\"\n",
        "k, pos_chars, filtered_vocab = 100, \"N\", True\n",
        "\n",
        "if filtered_vocab:\n",
        "    f_bw = [w.lower() for (w,t) in brown.tagged_words() if relevant(w,t,fd,k=k,pos_chars=pos_chars)]\n",
        "    f_fd = FreqDist(f_bw)\n",
        "    #print(len(bw))\n",
        "    f_sents = [[w.lower() for (w,t) in sent if relevant(w, t,f_fd,k=k,pos_chars=pos_chars)] for sent in tagged_sents]\n",
        "    f_sents = [s for s in f_sents if len(s)>1]\n",
        "    f_vocab=set(f_fd.keys())\n",
        "    f_V = len(f_vocab)\n",
        "    print(\"Filtered info: \",\"Sents\", len(f_sents),\"Vocab\", f_V, \"N\", sum(len(s) for s in f_sents))\n",
        "\n",
        "###############   End filtered vocab ###################################################################\n",
        "\n",
        "sents = [[w.lower() for (w,t) in sent] for sent in tagged_sents]\n",
        "vocab=set(fd.keys())\n",
        "sents = [s for s in sents if len(s)>1]\n",
        "num_events = len(sents)\n",
        "V = len(vocab)\n",
        "num_tokens = sum(len(s) for s in sents)\n",
        "#print(num_events,V)\n",
        "print(\"Base info: \",\"Sents\", num_events,\"Vocab\", V, \"N\", num_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfb3e3b0",
      "metadata": {
        "id": "bfb3e3b0"
      },
      "source": [
        "For ease of computation, the corpus has been filtered to include only nouns with frequency greater than 100 in the Brown corpus:\n",
        "\n",
        "Freq threshold for f_vocab, part of speech for f_vocab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94fc22a5",
      "metadata": {
        "id": "94fc22a5",
        "outputId": "e437e3b5-cb44-423b-c8f2-9060e7808d52"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(100, 'N')"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "k,pos_chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab97cfe5",
      "metadata": {
        "id": "ab97cfe5",
        "outputId": "4c59291e-96db-4b25-b405-4f4be4fe03ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "615"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(f_fd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "671c19dd",
      "metadata": {
        "id": "671c19dd",
        "outputId": "7af72180-e020-4378-d85c-0323f0b9a25c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "49815"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(fd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79f305c7",
      "metadata": {
        "id": "79f305c7",
        "outputId": "68593967-9c99-4583-d571-e54d8b6974c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19243"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(f_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c37c823",
      "metadata": {
        "id": "4c37c823",
        "outputId": "cf5dd43f-8a17-4adc-8873-81249fa21ca2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "57340"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(tagged_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3cbf838",
      "metadata": {
        "id": "f3cbf838",
        "outputId": "adecfe88-8fec-4e30-f2aa-987ce7647108"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1598"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fd[\"time\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49a57713",
      "metadata": {
        "id": "49a57713"
      },
      "source": [
        "Sentences of length 1 have been filtered too, meaning that frequencies of some words have gone down:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ee0a47f",
      "metadata": {
        "id": "8ee0a47f",
        "outputId": "8a28cd07-37f5-48f6-b1e5-cc1f074602c6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1555"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f_fd[\"time\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eed1e7b",
      "metadata": {
        "id": "8eed1e7b",
        "outputId": "4f9716a6-1d87-4bc4-faad-62fe80133d00"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "109"
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fd[\"language\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f7bfd41",
      "metadata": {
        "id": "5f7bfd41",
        "outputId": "b14d906c-09ff-47fb-c92f-363eb7a1af04"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "106"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f_fd[\"language\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efd7707f",
      "metadata": {
        "id": "efd7707f"
      },
      "source": [
        "Pick the 100 most words in the filetered vocabulary: These are the words to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aed99ab",
      "metadata": {
        "id": "3aed99ab",
        "outputId": "3722fed1-0641-4906-e0c0-c7b27d64c8c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n"
          ]
        }
      ],
      "source": [
        "target_list = f_fd.most_common(100)\n",
        "target_set = {w for (w,ct) in target_list}\n",
        "print(len(target_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e54213f",
      "metadata": {
        "id": "1e54213f"
      },
      "outputs": [],
      "source": [
        "f_sents[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6c4cfb2",
      "metadata": {
        "id": "d6c4cfb2",
        "outputId": "f285feb6-631e-4e23-a8ad-439a43960a98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['issue', 'state', 'sales']\n"
          ]
        }
      ],
      "source": [
        "#wd=\"work\"\n",
        "#sent = class_sents[wd][0]\n",
        "#if not wd== sent[0]:\n",
        "#   print(sent[:sent.index(wd)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3facd34e",
      "metadata": {
        "id": "3facd34e"
      },
      "source": [
        "Make a dictionary such that each key is a target word and\n",
        "the corresponding value is a list of brown \"filtered sentences\" containing that target word.\n",
        "Filter sentences in which the target word is the first word (such a sentence has no history that can be\n",
        "used to predict the target word)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7bfa8bb1",
      "metadata": {
        "id": "7bfa8bb1"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def make_class_sents(f_sents,target_set):\n",
        "    class_sents0 = defaultdict(list)\n",
        "    for sent in f_sents:\n",
        "        wds = target_set.intersection(sent)\n",
        "        for wd in wds:\n",
        "            if not wd == sent[0]:\n",
        "                class_sents0[wd].append(sent[:sent.index(wd)])\n",
        "    return {wd:sents for (wd,sents) in class_sents0.items() if len(sents)>100}\n",
        "\n",
        "#{wd:sents for (wd,sents) in class_sents0.items() if len(sents)>100}\n",
        "class_sents = make_class_sents(f_sents,target_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb4c6ba1",
      "metadata": {
        "id": "fb4c6ba1",
        "outputId": "42006195-5bb4-4538-8d49-86fb1494e0d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16026"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#len(class_sents[\"work\"])  276\n",
        "# len(class_sents) 91\n",
        "# so we actually only have 91 lcasses to predict\n",
        "# total_num_class_sents = sum(1 for sents in class_sents.values() for sent in sents)\n",
        "# total_num_class_sents 16026\n",
        "# so we have 16K  histories to split into"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9bee85f",
      "metadata": {
        "id": "e9bee85f"
      },
      "source": [
        "Wehave 91 target words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e920c3f7",
      "metadata": {
        "id": "e920c3f7",
        "outputId": "5ebf644c-42e8-44ab-e033-1c39abda1990"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "91"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(class_sents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b79ab341",
      "metadata": {
        "id": "b79ab341",
        "outputId": "7c481d50-23df-4985-de07-3a327a889f12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sents 16026\n",
            "words 28714\n"
          ]
        }
      ],
      "source": [
        "# number of sentences/wds in the filtered corpus\n",
        "\n",
        "print(\"sents\", sum(1 for sents in class_sents.values() for s in sents))\n",
        "print(\"words\", sum(1 for sents in class_sents.values() for s in sents for wd in s))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25a21025",
      "metadata": {
        "id": "25a21025"
      },
      "outputs": [],
      "source": [
        "#for s in sents[:25]:\n",
        "#    print(s)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc53e46a",
      "metadata": {
        "id": "cc53e46a"
      },
      "source": [
        "## Finding triggers  using Mutual Information (Rosenfeld 1994)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef878e8c",
      "metadata": {
        "id": "ef878e8c"
      },
      "source": [
        "## Mutual information calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "191acd46",
      "metadata": {
        "id": "191acd46"
      },
      "source": [
        "This code takes a while to run.  You can run it  if you like, or you can just use the value\n",
        "of `triggers`, the set of words being computed ion this section, which is assigned to the right set at the end of the **Trigger Vocab calculation** section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fef9d4b5",
      "metadata": {
        "id": "fef9d4b5"
      },
      "source": [
        "The comcept of a **trigger word**.\n",
        "\n",
        "For MI_ calculation: Each word gets assigned avector representing what sentences it has occurred in,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "908e7590",
      "metadata": {
        "id": "908e7590"
      },
      "outputs": [],
      "source": [
        "\n",
        "f_encoder = {wd:i for (i,wd) in enumerate(f_vocab)}\n",
        "\n",
        "\n",
        "def get_mi_word_vecs (sents,V,encoder):\n",
        "    vecs = np.zeros((V,len(sents)),dtype=int)\n",
        "    for (j,sent) in enumerate(sents):\n",
        "        cts = FreqDist(sent)\n",
        "        for (wd,ct) in cts.items():\n",
        "            vecs[encoder[wd],j] = ct\n",
        "    return vecs\n",
        "\n",
        "f_vecs = get_mi_word_vecs (f_sents,f_V,f_encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d66d60d",
      "metadata": {
        "id": "9d66d60d"
      },
      "source": [
        "Shape is V x len(f_sents)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b204de10",
      "metadata": {
        "id": "b204de10",
        "outputId": "4bb2d321-5934-454b-d26f-2a94d330ea4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(615, 19243)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f_vecs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "918188d2",
      "metadata": {
        "id": "918188d2"
      },
      "source": [
        "Now we can get the mutual information of two words by taking the mutual information score\n",
        "of their two word vectors.  So we compute all the pairwird MI scores for the filtered\n",
        "vocab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "5e1409bb",
      "metadata": {
        "id": "5e1409bb",
        "outputId": "db14b04d-b79a-4385-b9ec-eb8a727a9210",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tue Feb 24 22:32:44 2026\n",
            "Processing word 0 Tue Feb 24 22:32:44 2026\n",
            "Processing word 50 Tue Feb 24 22:33:41 2026\n",
            "Processing word 100 Tue Feb 24 22:34:33 2026\n",
            "Processing word 150 Tue Feb 24 22:35:20 2026\n",
            "Processing word 200 Tue Feb 24 22:36:02 2026\n",
            "Processing word 250 Tue Feb 24 22:36:40 2026\n",
            "Processing word 300 Tue Feb 24 22:37:13 2026\n",
            "Processing word 350 Tue Feb 24 22:37:41 2026\n",
            "Processing word 400 Tue Feb 24 22:38:04 2026\n",
            "Processing word 450 Tue Feb 24 22:38:23 2026\n",
            "Processing word 500 Tue Feb 24 22:38:37 2026\n",
            "Processing word 550 Tue Feb 24 22:38:45 2026\n",
            "Processing word 600 Tue Feb 24 22:38:49 2026\n",
            "Tue Feb 24 22:38:49 2026\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "f_enc_pairs = list(f_encoder.items())\n",
        "#mis0 = np.zeros((V,V))\n",
        "#mis = Triangular2DArray(mis0)\n",
        "#vecs[0]\n",
        "\n",
        "def get_mis (V,enc_pairs,vecs,batch_sz=50):\n",
        "    mis = np.zeros((V,V))\n",
        "    vecs_b = (vecs > 0)\n",
        "    for (idx,(wd,i)) in enumerate(enc_pairs):\n",
        "        if idx%batch_sz == 0:\n",
        "            print(f\"Processing word {idx} {time.ctime()}\")\n",
        "        for (wd2,j) in enc_pairs[idx:]:\n",
        "            mis[i,j] = mutual_info_score(vecs_b[i],vecs_b[j])\n",
        "    return mis\n",
        "\n",
        "print(time.ctime())\n",
        "mis = get_mis (f_V,f_enc_pairs,f_vecs)\n",
        "print(time.ctime())\n",
        "#word_vecs = vecs.sum(axis=0)\n",
        "#del vecs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "7770800d",
      "metadata": {
        "id": "7770800d",
        "outputId": "aeabbe4d-4f3e-4831-d93b-b03a8070a966",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.055420455303496284)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "mis[:,0].max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "e09ab9d5",
      "metadata": {
        "id": "e09ab9d5"
      },
      "outputs": [],
      "source": [
        "mi_max_vals = mis.max(axis=0)\n",
        "#mi_max_vals"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "0668c4f1",
      "metadata": {
        "id": "0668c4f1",
        "outputId": "e2181661-de99-44d1-fb84-cb50f526055a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.65837672e-01,\n",
              "       7.97898772e-05, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 3.23562035e-06, 0.00000000e+00, 0.00000000e+00,\n",
              "       0.00000000e+00, 1.88155503e-04, 1.46265317e-04, 1.45024858e-04,\n",
              "       0.00000000e+00, 2.04510321e-05, 3.38781227e-06, 2.40581049e-04,\n",
              "       2.54392436e-05, 5.37093416e-05, 0.00000000e+00, 1.04752790e-04,\n",
              "       1.65110380e-04, 7.12351438e-06, 0.00000000e+00, 1.02022092e-04,\n",
              "       0.00000000e+00, 0.00000000e+00, 3.66640059e-05, 1.10408400e-06,\n",
              "       0.00000000e+00, 6.81261880e-05, 2.24998338e-08, 0.00000000e+00,\n",
              "       1.12102489e-04, 9.50875569e-10, 0.00000000e+00])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "mis[f_encoder[\"man\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4caeafe0",
      "metadata": {
        "id": "4caeafe0"
      },
      "source": [
        "##  End MI calculations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d3513ea",
      "metadata": {
        "id": "1d3513ea"
      },
      "source": [
        "## Trigger vocab calculation\n",
        "\n",
        "Now for each vocab word find exactly one \"trigger\", another word strongly associated according to\n",
        "mutual information:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "f764cfde",
      "metadata": {
        "id": "f764cfde",
        "outputId": "6d9cdbc0-ba3e-48a2-ae12-9fdf99062894",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "150\n"
          ]
        }
      ],
      "source": [
        "def print_triggers (trigger_pairs, top_n=None):\n",
        "    if top_n is not None:\n",
        "        trigger_pairs = trigger_pairs[:top_n]\n",
        "    for (wd,trig,score) in trigger_pairs:\n",
        "        print(f\"{wd} {trig} {score:.5f}\")\n",
        "\n",
        "def get_triggers (mis, decoder, threshhold=.0002,verbose=False):\n",
        "    # Zero out self triggers for now\n",
        "    for i in range(mis.shape[0]):\n",
        "        mis[i,i] = 0\n",
        "    # Find the best trigger of word in V\n",
        "    trigger_pairs = [(decoder[idx],decoder[mis[idx].argmax()]) for idx in range(mis.shape[0])]\n",
        "    # as well as the MI scores\n",
        "    trigger_scores = [(decoder[idx],mis[idx].max()) for idx in range(mis.shape[0])]\n",
        "    # Apply threshhold\n",
        "    filtered_trigger_pairs= []\n",
        "    for (i,(wd, trig)) in enumerate(trigger_pairs):\n",
        "        score = trigger_scores[i][1]\n",
        "        if score > threshhold:\n",
        "            filtered_trigger_pairs.append((wd,trig,score))\n",
        "    # Sort by score\n",
        "    filtered_trigger_pairs.sort(key=lambda x: x[2])\n",
        "    if verbose:\n",
        "        print_triggers (filtered_trigger_pairs)\n",
        "    return filtered_trigger_pairs\n",
        "\n",
        "threshhold,verbose = .0002,False\n",
        "f_decoder = {i:wd for (wd,i) in f_encoder.items()}\n",
        "filtered_trigger_pairs = get_triggers (mis, f_decoder,threshhold=threshhold,verbose=verbose)\n",
        "(wds,triggers,scores) = zip(*filtered_trigger_pairs)\n",
        "triggers = set(triggers)\n",
        "print(len(triggers))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "1b95b437",
      "metadata": {
        "id": "1b95b437",
        "outputId": "8fba1c05-d6e0-4d79-9f1c-535acecc1f2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "women                     men                       0.00613\n",
            "girls                     boys                      0.00565\n",
            "wife                      husband                   0.00388\n",
            "view                      point                     0.00249\n",
            "eyes                      face                      0.00235\n",
            "tax                       year                      0.00211\n",
            "mother                    father                    0.00195\n",
            "costs                     cost                      0.00193\n",
            "market                    stock                     0.00188\n",
            "aid                       countries                 0.00187\n",
            "research                  development               0.00183\n",
            "door                      room                      0.00170\n",
            "college                   students                  0.00170\n",
            "children                  school                    0.00167\n",
            "service                   costs                     0.00164\n",
            "game                      ball                      0.00162\n",
            "forms                     list                      0.00158\n",
            "education                 schools                   0.00151\n",
            "plane                     image                     0.00148\n",
            "property                  tax                       0.00146\n",
            "surface                   temperature               0.00144\n",
            "death                     life                      0.00139\n",
            "weeks                     couple                    0.00139\n",
            "line                      image                     0.00137\n",
            "school                    schools                   0.00132\n",
            "staff                     member                    0.00125\n",
            "answer                    question                  0.00125\n",
            "development               program                   0.00124\n",
            "points                    line                      0.00121\n",
            "sales                     year                      0.00120\n",
            "car                       road                      0.00116\n",
            "rate                      industry                  0.00115\n",
            "fact                      matter                    0.00113\n",
            "teeth                     mouth                     0.00112\n",
            "members                   board                     0.00112\n",
            "peace                     world                     0.00112\n",
            "nations                   world                     0.00112\n",
            "subject                   matter                    0.00109\n",
            "child                     mother                    0.00107\n",
            "use                       land                      0.00106\n",
            "activity                  business                  0.00102\n",
            "war                       peace                     0.00098\n",
            "information               list                      0.00098\n",
            "form                      information               0.00096\n",
            "meaning                   words                     0.00095\n",
            "equipment                 farm                      0.00095\n",
            "spirit                    community                 0.00094\n",
            "family                    members                   0.00094\n",
            "court                     order                     0.00094\n",
            "freedom                   nations                   0.00093\n",
            "total                     number                    0.00089\n",
            "hall                      door                      0.00088\n",
            "room                      temperature               0.00087\n",
            "woman                     man                       0.00086\n",
            "analysis                  cost                      0.00086\n",
            "months                    cent                      0.00085\n",
            "system                    population                0.00083\n",
            "boy                       girl                      0.00082\n",
            "center                    growth                    0.00082\n",
            "head                      hair                      0.00082\n",
            "class                     service                   0.00081\n",
            "policy                    stage                     0.00081\n",
            "plant                     equipment                 0.00080\n",
            "water                     temperature               0.00076\n",
            "state                     law                       0.00076\n",
            "money                     lot                       0.00076\n",
            "spring                    summer                    0.00075\n",
            "report                    trial                     0.00074\n",
            "industry                  increase                  0.00073\n",
            "ideas                     literature                0.00073\n",
            "activities                program                   0.00073\n",
            "operation                 cost                      0.00072\n",
            "face                      hair                      0.00071\n",
            "house                     front                     0.00070\n",
            "month                     hours                     0.00069\n",
            "miles                     distance                  0.00069\n",
            "image                     lines                     0.00069\n",
            "hair                      back                      0.00069\n",
            "window                    room                      0.00065\n",
            "range                     temperature               0.00065\n",
            "way                       life                      0.00062\n",
            "effort                    development               0.00061\n",
            "factors                   control                   0.00060\n",
            "community                 programs                  0.00060\n",
            "night                     morning                   0.00060\n",
            "years                     age                       0.00059\n",
            "word                      meaning                   0.00059\n",
            "business                  year                      0.00059\n",
            "power                     influence                 0.00059\n",
            "responsibility            government                0.00059\n",
            "support                   program                   0.00057\n",
            "persons                   person                    0.00056\n",
            "schools                   cities                    0.00056\n",
            "degree                    factors                   0.00056\n",
            "pressure                  temperature               0.00056\n",
            "study                     literature                0.00055\n",
            "period                    years                     0.00055\n",
            "science                   future                    0.00054\n",
            "difference                function                  0.00054\n",
            "leaders                   party                     0.00054\n",
            "production                equipment                 0.00054\n",
            "picture                   plane                     0.00053\n",
            "process                   stage                     0.00053\n",
            "story                     mother                    0.00053\n",
            "function                  value                     0.00053\n",
            "blood                     heart                     0.00053\n",
            "ground                    feet                      0.00053\n",
            "experience                spirit                    0.00052\n",
            "parts                     country                   0.00052\n",
            "light                     reaction                  0.00052\n",
            "paper                     top                       0.00052\n",
            "figure                    temperature               0.00052\n",
            "thought                   ideas                     0.00052\n",
            "street                    corner                    0.00051\n",
            "letter                    son                       0.00050\n",
            "town                      meeting                   0.00050\n",
            "principle                 plan                      0.00050\n",
            "name                      company                   0.00049\n",
            "cent                      total                     0.00049\n",
            "hands                     feet                      0.00049\n",
            "hours                     week                      0.00048\n",
            "labor                     areas                     0.00048\n",
            "efforts                   support                   0.00048\n",
            "morning                   sun                       0.00048\n",
            "job                       school                    0.00048\n",
            "population                total                     0.00047\n",
            "point                     image                     0.00047\n",
            "order                     lines                     0.00047\n",
            "systems                   development               0.00046\n",
            "president                 company                   0.00046\n",
            "list                      step                      0.00046\n",
            "love                      story                     0.00046\n",
            "history                   literature                0.00046\n",
            "cities                    year                      0.00046\n",
            "reaction                  temperature               0.00046\n",
            "hour                      miles                     0.00046\n",
            "corner                    car                       0.00045\n",
            "sense                     responsibility            0.00045\n",
            "voice                     face                      0.00045\n",
            "values                    data                      0.00045\n",
            "farm                      year                      0.00045\n",
            "son                       father                    0.00045\n",
            "forces                    defense                   0.00045\n",
            "mind                      ideas                     0.00045\n",
            "sound                     water                     0.00045\n",
            "program                   defense                   0.00044\n",
            "return                    year                      0.00044\n",
            "defense                   programs                  0.00044\n",
            "effect                    reaction                  0.00044\n",
            "side                      road                      0.00044\n",
            "government                state                     0.00043\n",
            "volume                    top                       0.00043\n",
            "statement                 board                     0.00042\n",
            "course                    student                   0.00042\n",
            "plan                      year                      0.00042\n",
            "cars                      car                       0.00042\n",
            "problem                   approach                  0.00042\n",
            "faith                     life                      0.00041\n",
            "record                    year                      0.00040\n",
            "stage                     state                     0.00040\n",
            "steps                     door                      0.00040\n",
            "problems                  attention                 0.00040\n",
            "thing                     sort                      0.00039\n",
            "need                      schools                   0.00039\n",
            "basis                     cost                      0.00039\n",
            "amount                    activity                  0.00039\n",
            "nature                    man                       0.00039\n",
            "world                     progress                  0.00039\n",
            "wall                      feet                      0.00038\n",
            "men                       head                      0.00038\n",
            "color                     hair                      0.00038\n",
            "areas                     man                       0.00038\n",
            "air                       surface                   0.00038\n",
            "area                      schools                   0.00038\n",
            "stock                     trade                     0.00037\n",
            "treatment                 child                     0.00037\n",
            "days                      week                      0.00036\n",
            "radio                     surface                   0.00036\n",
            "ways                      life                      0.00036\n",
            "life                      case                      0.00036\n",
            "land                      miles                     0.00036\n",
            "law                       states                    0.00035\n",
            "moment                    mind                      0.00035\n",
            "means                     force                     0.00035\n",
            "hand                      mouth                     0.00035\n",
            "girl                      man                       0.00035\n",
            "methods                   research                  0.00035\n",
            "day                       year                      0.00035\n",
            "student                   year                      0.00035\n",
            "effects                   reaction                  0.00034\n",
            "students                  man                       0.00034\n",
            "body                      face                      0.00034\n",
            "increase                  year                      0.00033\n",
            "data                      students                  0.00033\n",
            "number                    value                     0.00033\n",
            "charge                    trial                     0.00033\n",
            "sun                       summer                    0.00032\n",
            "front                     back                      0.00032\n",
            "trade                     business                  0.00032\n",
            "questions                 science                   0.00032\n",
            "time                      students                  0.00031\n",
            "length                    feet                      0.00031\n",
            "table                     words                     0.00031\n",
            "respect                   principle                 0.00030\n",
            "design                    color                     0.00030\n",
            "audience                  effect                    0.00029\n",
            "language                  literature                0.00029\n",
            "end                       length                    0.00029\n",
            "field                     world                     0.00029\n",
            "party                     government                0.00028\n",
            "growth                    age                       0.00028\n",
            "knowledge                 language                  0.00028\n",
            "level                     temperature               0.00028\n",
            "public                    increase                  0.00027\n",
            "right                     states                    0.00027\n",
            "people                    week                      0.00027\n",
            "states                    cities                    0.00027\n",
            "century                   literature                0.00027\n",
            "interest                  man                       0.00027\n",
            "home                      case                      0.00027\n",
            "results                   study                     0.00027\n",
            "influence                 literature                0.00027\n",
            "afternoon                 home                      0.00027\n",
            "art                       man                       0.00027\n",
            "progress                  countries                 0.00026\n",
            "groups                    students                  0.00026\n",
            "existence                 order                     0.00026\n",
            "couple                    minutes                   0.00025\n",
            "floor                     bed                       0.00025\n",
            "heart                     feeling                   0.00025\n",
            "terms                     man                       0.00025\n",
            "material                  surface                   0.00025\n",
            "society                   population                0.00025\n",
            "pattern                   growth                    0.00025\n",
            "husband                   friends                   0.00025\n",
            "mouth                     head                      0.00025\n",
            "reasons                   cost                      0.00025\n",
            "sort                      person                    0.00024\n",
            "strength                  forces                    0.00024\n",
            "words                     year                      0.00024\n",
            "city                      center                    0.00024\n",
            "man                       programs                  0.00024\n",
            "countries                 man                       0.00024\n",
            "change                    pressure                  0.00024\n",
            "theory                    law                       0.00024\n",
            "cases                     state                     0.00024\n",
            "top                       right                     0.00023\n",
            "purpose                   section                   0.00023\n",
            "member                    board                     0.00022\n",
            "meeting                   board                     0.00022\n",
            "type                      blood                     0.00022\n",
            "summer                    home                      0.00022\n",
            "size                      man                       0.00022\n",
            "matter                    time                      0.00022\n",
            "kind                      man                       0.00022\n",
            "performance               season                    0.00022\n",
            "force                     pressure                  0.00022\n",
            "letters                   numbers                   0.00021\n",
            "friend                    man                       0.00021\n",
            "part                      responsibility            0.00021\n",
            "evening                   hours                     0.00021\n",
            "action                    place                     0.00021\n",
            "importance                religion                  0.00021\n",
            "question                  year                      0.00021\n",
            "man's                     religion                  0.00021\n",
            "church                    year                      0.00021\n",
            "evidence                  trial                     0.00021\n",
            "office                    return                    0.00021\n",
            "movement                  man                       0.00020\n",
            "position                  day                       0.00020\n",
            "trouble                   couple                    0.00020\n",
            "step                      feet                      0.00020\n",
            "person                    people                    0.00020\n",
            "method                    chance                    0.00020\n",
            "organization              party                     0.00020\n"
          ]
        }
      ],
      "source": [
        "#(wds00,triggers00,scores00) = zip(*filtered_trigger_pairs)\n",
        "for (w,t,s) in filtered_trigger_pairs[::-1]:\n",
        "    print(f\"{w:<25} {t:<25} {s:.5f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fd18392",
      "metadata": {
        "id": "9fd18392"
      },
      "source": [
        "An important limitation of **mutual information**:  These words have been\n",
        "discovered because the occurrence of one of them in a sentence increases ythe likelihood oif its partner occurring in a sentence.  So they're here bnecause they ocurred in the **same** sentence often,\n",
        "not because they occurred in **similar** sentences often.  We will return to this issue and offer a solution when we consider embeddings models of word meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8845147d",
      "metadata": {
        "id": "8845147d"
      },
      "source": [
        "## Must execute the next code cell"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2f07046",
      "metadata": {
        "id": "a2f07046"
      },
      "source": [
        "You can run the code to find the correct value for `triggers`, or you can just use the value for `triggers` set in the next cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "4f0d474f",
      "metadata": {
        "id": "4f0d474f"
      },
      "outputs": [],
      "source": [
        "triggers = {'action',\n",
        " 'aid',\n",
        " 'amount',\n",
        " 'analysis',\n",
        " 'answer',\n",
        " 'areas',\n",
        " 'attention',\n",
        " 'bed',\n",
        " 'blood',\n",
        " 'board',\n",
        " 'boy',\n",
        " 'boys',\n",
        " 'business',\n",
        " 'car',\n",
        " 'case',\n",
        " 'cent',\n",
        " 'child',\n",
        " 'college',\n",
        " 'community',\n",
        " 'company',\n",
        " 'corner',\n",
        " 'cost',\n",
        " 'countries',\n",
        " 'couple',\n",
        " 'court',\n",
        " 'day',\n",
        " 'defense',\n",
        " 'distance',\n",
        " 'door',\n",
        " 'effect',\n",
        " 'equipment',\n",
        " 'extent',\n",
        " 'face',\n",
        " 'fact',\n",
        " 'factors',\n",
        " 'faith',\n",
        " 'floor',\n",
        " 'forces',\n",
        " 'form',\n",
        " 'forms',\n",
        " 'freedom',\n",
        " 'friends',\n",
        " 'front',\n",
        " 'function',\n",
        " 'future',\n",
        " 'game',\n",
        " 'government',\n",
        " 'growth',\n",
        " 'hair',\n",
        " 'hands',\n",
        " 'head',\n",
        " 'heart',\n",
        " 'history',\n",
        " 'home',\n",
        " 'hours',\n",
        " 'husband',\n",
        " 'ideas',\n",
        " 'image',\n",
        " 'increase',\n",
        " 'industry',\n",
        " 'influence',\n",
        " 'issue',\n",
        " 'labor',\n",
        " 'land',\n",
        " 'language',\n",
        " 'law',\n",
        " 'leaders',\n",
        " 'length',\n",
        " 'letter',\n",
        " 'level',\n",
        " 'life',\n",
        " 'line',\n",
        " 'literature',\n",
        " 'man',\n",
        " 'market',\n",
        " 'material',\n",
        " 'meaning',\n",
        " 'means',\n",
        " 'meeting',\n",
        " 'member',\n",
        " 'members',\n",
        " 'method',\n",
        " 'mind',\n",
        " 'money',\n",
        " 'month',\n",
        " 'months',\n",
        " 'morning',\n",
        " 'mother',\n",
        " 'mouth',\n",
        " 'nations',\n",
        " 'number',\n",
        " 'numbers',\n",
        " 'others',\n",
        " 'paper',\n",
        " 'parts',\n",
        " 'party',\n",
        " 'persons',\n",
        " 'piece',\n",
        " 'plane',\n",
        " 'point',\n",
        " 'policy',\n",
        " 'pool',\n",
        " 'population',\n",
        " 'pressure',\n",
        " 'principle',\n",
        " 'problem',\n",
        " 'production',\n",
        " 'program',\n",
        " 'programs',\n",
        " 'progress',\n",
        " 'property',\n",
        " 'range',\n",
        " 'reaction',\n",
        " 'religion',\n",
        " 'research',\n",
        " 'respect',\n",
        " 'sales',\n",
        " 'school',\n",
        " 'schools',\n",
        " 'science',\n",
        " 'season',\n",
        " 'situation',\n",
        " 'society',\n",
        " 'son',\n",
        " 'sound',\n",
        " 'spirit',\n",
        " 'state',\n",
        " 'statement',\n",
        " 'street',\n",
        " 'student',\n",
        " 'summer',\n",
        " 'sun',\n",
        " 'surface',\n",
        " 'systems',\n",
        " 'tax',\n",
        " 'temperature',\n",
        " 'terms',\n",
        " 'thing',\n",
        " 'town',\n",
        " 'treatment',\n",
        " 'truth',\n",
        " 'value',\n",
        " 'values',\n",
        " 'war',\n",
        " 'water',\n",
        " 'way',\n",
        " 'ways',\n",
        " 'women',\n",
        " 'world',\n",
        " 'years'}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d93d9d1",
      "metadata": {
        "id": "6d93d9d1"
      },
      "source": [
        "This should evaluate to 150 to get the results I want you to reproduce:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d585fe4a",
      "metadata": {
        "scrolled": true,
        "id": "d585fe4a",
        "outputId": "da0ec049-10b0-47e2-f204-18d9f617762b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "150"
            ]
          },
          "execution_count": 290,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(triggers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "597fca2f",
      "metadata": {
        "id": "597fca2f"
      },
      "source": [
        "## Make the Korpus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a182c89d",
      "metadata": {
        "id": "a182c89d"
      },
      "source": [
        "To make a corpus you will call the function `prepare_korpus` defined and called in the few code cells.\n",
        "\n",
        "The first thing it will do is convert the filtered corpus into a set of history, predicted_word\n",
        "pairs.  This is done in the function `make_class_sents`, which takes as its arguments\n",
        "the filtered corpus and the set of class words (`class_set`)\n",
        "which are all frequent words\n",
        "selected to guarantee there would be enough examples of each predicted word to make reasonable training possible, even in  this small dataset.  \n",
        "\n",
        "The function `make_class_sents` returns  a dictionary `class_sents` which has t he following structure:\n",
        "\n",
        "```python\n",
        "class_wd |-> histories\n",
        "```\n",
        "\n",
        "where each `class_wd` is a word to be predicted and `histories` is a list\n",
        "of (filtered) histories for which_class_wd is the next word.\n",
        "\n",
        "Here's an example of the contents:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "a0668798",
      "metadata": {
        "id": "a0668798",
        "outputId": "9ed86753-7591-4cda-b03a-021665852668",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['law'],\n",
              " ['police', 'trial'],\n",
              " ['today', 'business'],\n",
              " ['administration', 'policy'],\n",
              " ['city'],\n",
              " ['night', 'study', 'changes'],\n",
              " ['group', 'mind'],\n",
              " ['sales', 'state', 'tax'],\n",
              " ['right'],\n",
              " ['scene'],\n",
              " ['defense'],\n",
              " ['game'],\n",
              " ['sun'],\n",
              " ['points', 'years', 'school'],\n",
              " ['boy'],\n",
              " ['efforts'],\n",
              " ['party'],\n",
              " ['home'],\n",
              " ['evening'],\n",
              " ['market', 'years']]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "class_sents = make_class_sents(f_sents,target_set)\n",
        "# Histories followed by the word \"time\"\n",
        "class_sents[\"time\"][:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb111a94",
      "metadata": {
        "id": "eb111a94"
      },
      "source": [
        "If we mush all the histories associated with all the target words, there are 16,026 histories.  That's how many histories we will train on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "c9e23825",
      "metadata": {
        "id": "c9e23825",
        "outputId": "92c1c99a-04ce-44cb-8a3f-ebf61b1a28a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16026"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "sum(len(sents) for sents in class_sents.values())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48bd0b15",
      "metadata": {
        "id": "48bd0b15"
      },
      "source": [
        "The `class_sents` dictionary is then used to create `korpus`, the array representation of all 16,026\n",
        "histories,  and `Y` the corresponding 16,026 words to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "4e051773",
      "metadata": {
        "id": "4e051773"
      },
      "outputs": [],
      "source": [
        "def prepare_korpus (f_sents, target_set, active_triggers):\n",
        "    # Do creation of class_sents here because this code destructively modifies the sent lists\n",
        "    # class sents is a dictionary:  class_wd |-> histories\n",
        "    # where each eclass_d is a wd to be predicted and hsitories is a list\n",
        "    # (filetered) histories for which_class_wd is the next word.\n",
        "    class_sents = make_class_sents(f_sents,target_set)\n",
        "    korpus0 = []\n",
        "    final_vocab0 = set()\n",
        "    for cls_wd,sents in class_sents.items():\n",
        "        for sent in sents:\n",
        "            sent[-1] = sent[-1] + \"_b\"\n",
        "            final_vocab0.add(sent[-1])\n",
        "            korpus0.append((sent,cls_wd))\n",
        "\n",
        "    final_vocab = final_vocab0 | active_triggers\n",
        "\n",
        "    #########   FINAL  PASS: Vectorize; Create korpus and Y   ########################\n",
        "    final_sample_sz, final_V = (len(korpus0),len(final_vocab))\n",
        "    korpus = np.zeros((final_sample_sz, final_V))\n",
        "    #final_dim = final_V + len(active_triggers)\n",
        "    final_encoder = {wd:i for (i,wd) in enumerate(final_vocab)}\n",
        "\n",
        "    trigger_ct = 0\n",
        "    Y = []    #np.array((final_sample_sz,))\n",
        "    for (i,(sent,cls_wd)) in enumerate(korpus0):\n",
        "        bigram_wd = sent[-1]\n",
        "        #print(bigram_wd,final_encoder[bigram_wd])\n",
        "        korpus[i,final_encoder[bigram_wd]] += 1\n",
        "        these_triggers = active_triggers.intersection(sent)\n",
        "        for trig in these_triggers:\n",
        "            trigger_ct += 1\n",
        "            korpus[i,final_encoder[trig]] += 1\n",
        "        Y.append(cls_wd)\n",
        "    Y=np.array(Y)\n",
        "    ##########  END FINAL  PASS  #######################\n",
        "    print(f\"Korpus created:: korpus shape: {korpus.shape}  Y shape: {Y.shape} triggers used: {trigger_ct}\")\n",
        "    return korpus, korpus0, Y\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "b80b73b9",
      "metadata": {
        "id": "b80b73b9",
        "outputId": "dba94856-6dc1-4016-8611-bd9272112456",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Korpus created:: korpus shape: (16026, 508)  Y shape: (16026,) triggers used: 5511\n"
          ]
        }
      ],
      "source": [
        "# Make the corpus.  Must execute this cell.\n",
        "korpus, korpus0, Y = prepare_korpus (f_sents, target_set, triggers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e64e911",
      "metadata": {
        "id": "1e64e911"
      },
      "source": [
        "##  Number of features for the base model (with triggers)\n",
        "\n",
        "The number of features for this model is 508."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cd80d0b",
      "metadata": {
        "id": "1cd80d0b"
      },
      "source": [
        "The `prepare_corpus` function returns three things:\n",
        "\n",
        "1. `korpus`:  16,026 histories from the Brown corpus encoded as a 16,026x508 array, where 508 is the number of dimensions in a history vector, the encoded representation of a history.\n",
        "2. `Y`:  the 16,026 target words for those history vectors. These are the words our language model will try to predict. A target word is always a word that occurred later in the same sentence as the words in its history in the orginal Brown corpus.\n",
        "2.  `big_korpus0`: a sequence of history, target pairs represented as words."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9af9d23",
      "metadata": {
        "id": "e9af9d23"
      },
      "source": [
        "The histories in `korpus0` differ from the histories in `class_sents` only in that they're in a flat list\n",
        "and the last words have been modified to have \"_b\" on them.  This is because one of the best trigger words for any word is itself:  once a content word with unigram probability $p$ occurs in any document the likelihood of its occurring in the rest of the document is higher than $p$. This property is sometimes called **burstiness.**\n",
        "To allow for the possibility for the same word to occur in\n",
        "the history both as the last word (the bigram prefix) and earlier (as a trigger), trigger words\n",
        "and words in final position in a history have different features; `korpus0` was a convenience\n",
        "in building the training data, but will play no role in training.  It does however\n",
        "help when we have questions about how a particular histopry gave rise to its feature reprersentation in\n",
        "`korpus`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "9c3fd8d9",
      "metadata": {
        "id": "9c3fd8d9",
        "outputId": "eeff5351-2d6e-4494-93a2-c3a9d175f392",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['food', 'family_b'], 'place')"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "korpus0[12]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fad804ea",
      "metadata": {
        "id": "fad804ea"
      },
      "source": [
        "Here is part of a row from `korpus`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "681b0429",
      "metadata": {
        "id": "681b0429"
      },
      "source": [
        "It  is a sparse matrix, mostly 0s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "3b938fd3",
      "metadata": {
        "id": "3b938fd3",
        "outputId": "e0ff20d5-cbf4-4271-83e3-81c39d8f5858",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "korpus[12,:25]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd7335d3",
      "metadata": {
        "id": "cd7335d3"
      },
      "source": [
        "But each row will have at least one non-zero value in there because the bigram word (with a \"_b\" at\n",
        "the end) will always be an encoded feature of the history."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "fb32828e",
      "metadata": {
        "id": "fb32828e",
        "outputId": "a4005535-3656-498a-ae4b-818af10cf70b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "korpus[12].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f98e39b",
      "metadata": {
        "id": "8f98e39b"
      },
      "source": [
        "Consider how the word sequence in `korpus0[12]` is related to the 1D array `korpus[12]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "58f9f66c",
      "metadata": {
        "id": "58f9f66c",
        "outputId": "dc32ed43-1351-477e-d2e7-b9f5c65453da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['food', 'family_b'], 'place')"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "korpus0[12]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d774ef6",
      "metadata": {
        "id": "8d774ef6"
      },
      "source": [
        "There is only one active feature in row 12, because \"food\" is not a trigger word. Since tt's not in bigram possition\n",
        "and it's not a trigger, we ignore it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "2f5cbe5b",
      "metadata": {
        "id": "2f5cbe5b",
        "outputId": "8b0154d3-bdb0-4c4c-8afd-95999fc971d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "\"food\"  in triggers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddfe6254",
      "metadata": {
        "id": "ddfe6254"
      },
      "source": [
        "Let's find a more interesting example.  The row with the greatest number of active features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "2bd3ee12",
      "metadata": {
        "scrolled": true,
        "id": "2bd3ee12",
        "outputId": "f0b50445-b662-4566-bfab-e187ad7666b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(1377)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "korpus.sum(axis=1).argmax()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d8f6a34",
      "metadata": {
        "id": "9d8f6a34"
      },
      "source": [
        "When there is a no trigger word the history for the\n",
        "word prediction has only one non zero feature, the feature\n",
        "for the immediately preceding word (the bigram feature).\n",
        "When there is also a single trigger word, there are two nonzero features\n",
        "features.  History `1484` has 6 trigger words in additionto the bigram word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "eaaf797d",
      "metadata": {
        "scrolled": true,
        "id": "eaaf797d",
        "outputId": "bb525a17-e4b8-4af0-8ec9-7879e551ec8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "korpus[1484].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26d5f756",
      "metadata": {
        "id": "26d5f756"
      },
      "source": [
        "This history and target word (word-to-predict, or class) of sample 1484:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "5d8ce1e3",
      "metadata": {
        "id": "5d8ce1e3",
        "outputId": "758ee382-5b47-4056-bf7e-6b72da61fcbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['study', 'end_b'], 'man')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "korpus0[1484]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "a73cf970",
      "metadata": {
        "scrolled": true,
        "id": "a73cf970",
        "outputId": "4786d18d-194c-4a6d-ce51-09aa12552b99",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "set()"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "ts_1484 = triggers.intersection(korpus0[1484][0])\n",
        "ts_1484"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9797034",
      "metadata": {
        "id": "a9797034"
      },
      "source": [
        "Alas none of these trigger words has much of an association  with the given target word:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "e5aea988",
      "metadata": {
        "id": "e5aea988"
      },
      "outputs": [],
      "source": [
        "#  Attn:  Re-evaluate this cell only if you have computed themi scores in thsi notebook session.\n",
        "man_vec = mis[f_encoder[\"man\"]]\n",
        "for t in ts_1484:\n",
        "    print(f\"{t} {man_vec[f_encoder[t]]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "e6f95a16",
      "metadata": {
        "id": "e6f95a16"
      },
      "outputs": [],
      "source": [
        "sents = [[w.lower() for (w,t) in sent] for sent in tagged_sents]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d660c1f",
      "metadata": {
        "id": "6d660c1f"
      },
      "source": [
        "Here's the original sentence from the corpus.  The target word `man` is the last word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "8144e112",
      "metadata": {
        "scrolled": false,
        "id": "8144e112",
        "outputId": "f3f62292-ef63-45e2-e198-da197a990914",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\" \".join([w for s in sents for w in s if len(ts_1484.intersection(s))==6]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba42a59a",
      "metadata": {
        "id": "ba42a59a"
      },
      "source": [
        "The average row sum is approximately 1.34,\n",
        "which means there are a significant number of trigger words in `korpus`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "e20f1527",
      "metadata": {
        "id": "e20f1527",
        "outputId": "8606236c-e89d-4241-ea9d-c73937d7a38d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(1.3438786971171846)"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "Ss = korpus.sum(axis=1)\n",
        "Ss.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f25645b",
      "metadata": {
        "id": "0f25645b"
      },
      "source": [
        "##  Training the Logistic Regression classifier (with triggers)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c26dd5b8",
      "metadata": {
        "id": "c26dd5b8"
      },
      "source": [
        "This takes a little time, see the wall time printouts below from my Mac.  Your mileage may vary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "06567839",
      "metadata": {
        "id": "06567839",
        "outputId": "dc418148-de84-49ba-e34f-20313bda5ed5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Feb 24 22:56:36 2026\n",
            "Tue Feb 24 22:57:43 2026\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "import time\n",
        "\n",
        "#     LogisticRegression(penalty='deprecated',\n",
        "#                        C=1.0, l1_ratio=0.0, dual=False,\n",
        "#                         tol=0.0001, fit_intercept=True,\n",
        "#                         intercept_scaling=1, class_weight=None,\n",
        "#                         random_state=None, solver='lbfgs',\n",
        "#                         max_iter=100, verbose=0, warm_start=False, n_jobs=None)\n",
        "\n",
        "# we do want solver - 'saga' (sag wil also work also gd for large datasets)\n",
        "# and l1_ratio = 0, Also it's a good multiclass algorithm.\n",
        "lrc = LogisticRegression(solver=\"saga\")\n",
        "print(time.ctime())\n",
        "#  Training command\n",
        "lrc.fit(korpus,Y)\n",
        "print(time.ctime())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "X0_tokens = [feat_list for (feat_list, _) in korpus0]\n",
        "\n",
        "X0_text = [\" \".join(feats) for feats in X0_tokens]\n",
        "\n",
        "vec0 = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
        "X0 = vec0.fit_transform(X0_text)"
      ],
      "metadata": {
        "id": "IPgWJbe16vcR"
      },
      "id": "IPgWJbe16vcR",
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "lrc_bigram = LogisticRegression(max_iter=1000)\n",
        "lrc_bigram.fit(X0, Y)\n",
        "\n",
        "probs_bigram = lrc_bigram.predict_proba(X0)\n",
        "\n",
        "class_idx_bigram = np.searchsorted(lrc_bigram.classes_, Y)\n",
        "correct_probs_bigram = probs_bigram[np.arange(len(Y)), class_idx_bigram]\n",
        "\n",
        "perp_bigram = np.exp(-np.mean(np.log(correct_probs_bigram)))\n",
        "perp_bigram"
      ],
      "metadata": {
        "id": "MXJrxzuZ8Az2",
        "outputId": "a2942fe1-5558-4e0c-a128-983947399a0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "MXJrxzuZ8Az2",
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(30.08564140162269)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigger_model_is_better = perp1 < perp_bigram\n",
        "trigger_model_is_better"
      ],
      "metadata": {
        "id": "VavQ_doI8LUm",
        "outputId": "c0dedc40-a0d3-400d-f0be-2dcd558ce23b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "VavQ_doI8LUm",
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.False_"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_features1 = korpus.shape[1]\n",
        "num_features1"
      ],
      "metadata": {
        "id": "babUJhos5mKj",
        "outputId": "8b626832-f41a-457f-c2ef-816a7f8d8863",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "babUJhos5mKj",
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "508"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "probs = lrc.predict_proba(korpus)\n",
        "\n",
        "class_indices = np.searchsorted(lrc.classes_, Y)\n",
        "\n",
        "correct_probs = probs[np.arange(len(Y)), class_indices]\n",
        "\n",
        "perp1 = np.exp(-np.mean(np.log(correct_probs)))\n",
        "\n",
        "perp1"
      ],
      "metadata": {
        "id": "53qtZx1P4g2G",
        "outputId": "780d3872-e64d-40e1-9f97-61dd13ad8601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "53qtZx1P4g2G",
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(37.20636413221655)"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_features_bigram = X0.shape[1]\n",
        "num_features_bigram"
      ],
      "metadata": {
        "id": "Uw5mpwht8h2i",
        "outputId": "bc447189-7c1f-4749-b0ea-29d0b2ea8ef8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Uw5mpwht8h2i",
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "716"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perp_uniform = len(lrc.classes_)\n",
        "perp_uniform"
      ],
      "metadata": {
        "id": "awrDmY6E9JKU",
        "outputId": "07def967-0b65-48de-f4ca-0ef8323118d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "awrDmY6E9JKU",
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "y_pred = lrc.predict(korpus)\n",
        "\n",
        "precisions = precision_score(Y, y_pred, average=None)\n",
        "\n",
        "best_index = np.argmax(precisions)\n",
        "\n",
        "max_precision_word = lrc.classes_[best_index]\n",
        "\n",
        "max_precision_word"
      ],
      "metadata": {
        "id": "mrkWYLYE9WfB",
        "outputId": "ca4da517-4b52-400e-dc46-06510e027bd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "id": "mrkWYLYE9WfB",
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.str_('times')"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "160084c7",
      "metadata": {
        "id": "160084c7"
      },
      "source": [
        "## Predictions with the model\n",
        "\n",
        "There are two different models that arise in the assignment questions, one with trigger words, one\n",
        "without (the bigrams only model).  The model you are being gven in this code help notebook is\n",
        "the model with triggers.\n",
        "\n",
        "All the questions on the assignment involve predictions made **on the training set**. We can do two kinds\n",
        "of predictions with our trained `lrc` model, predicting probabilities for the training set\n",
        "and predicting words (the classes our classifier predicts are words) or predicting probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "d1697348",
      "metadata": {
        "id": "d1697348"
      },
      "outputs": [],
      "source": [
        "predicted_words = lrc.predict(korpus)\n",
        "predicted_probs = lrc.predict_proba(korpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "833c80fc",
      "metadata": {
        "id": "833c80fc",
        "outputId": "b6202a41-6634-4f27-e438-f11251174041",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['years', 'year', 'man', 'time', 'year', 'school', 'place', 'place',\n",
              "       'day', 'world'], dtype='<U11')"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "predicted_words[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bf60a7a",
      "metadata": {
        "id": "3bf60a7a"
      },
      "source": [
        "One predicted word for every history in the training set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "01242b02",
      "metadata": {
        "id": "01242b02",
        "outputId": "bb36d1b7-73e2-48e0-9548-5714d74c3511",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16026,)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "predicted_words.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24bd9b82",
      "metadata": {
        "id": "24bd9b82"
      },
      "source": [
        "Our `predicted_probs` is a 2D array with shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "52f7066d",
      "metadata": {
        "id": "52f7066d",
        "outputId": "3c5bc9e8-1e8e-4141-87cd-50b356183b68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16026, 91)"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ],
      "source": [
        "predicted_probs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7c56a90",
      "metadata": {
        "id": "e7c56a90"
      },
      "source": [
        "The reason for this should become clear in the discussion of `predict_proba` in the following section."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c102e352",
      "metadata": {
        "id": "c102e352"
      },
      "source": [
        "##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "642eb2af",
      "metadata": {
        "id": "642eb2af"
      },
      "source": [
        "## Example of using classifiers in scikit learn (sprinkled liberally with hints for the assignment)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "190e8960",
      "metadata": {
        "id": "190e8960"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from inspect import signature"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9893bd71",
      "metadata": {
        "id": "9893bd71"
      },
      "source": [
        "### The data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "84234db6",
      "metadata": {
        "id": "84234db6",
        "outputId": "1925baf8-6804-438a-d723-b8115c4c47c7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism',\n",
              " 'comp.graphics',\n",
              " 'comp.os.ms-windows.misc',\n",
              " 'comp.sys.ibm.pc.hardware',\n",
              " 'comp.sys.mac.hardware',\n",
              " 'comp.windows.x',\n",
              " 'misc.forsale',\n",
              " 'rec.autos',\n",
              " 'rec.motorcycles',\n",
              " 'rec.sport.baseball',\n",
              " 'rec.sport.hockey',\n",
              " 'sci.crypt',\n",
              " 'sci.electronics',\n",
              " 'sci.med',\n",
              " 'sci.space',\n",
              " 'soc.religion.christian',\n",
              " 'talk.politics.guns',\n",
              " 'talk.politics.mideast',\n",
              " 'talk.politics.misc',\n",
              " 'talk.religion.misc']"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ],
      "source": [
        "newsgroups = fetch_20newsgroups()\n",
        "newsgroups['target_names']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "2e11f74c",
      "metadata": {
        "id": "2e11f74c"
      },
      "outputs": [],
      "source": [
        "# We want a multiclass problem, so pick three of the 20 categories\n",
        "categories = ['alt.atheism', 'sci.space','comp.graphics']\n",
        "newsgroups_train = fetch_20newsgroups(subset='train',\n",
        "                                     categories=categories)\n",
        "newsgroups_test = fetch_20newsgroups(subset='test',\n",
        "                                     categories=categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "6d69d09f",
      "metadata": {
        "scrolled": true,
        "id": "6d69d09f",
        "outputId": "c1fdd3db-a9b6-4de8-fd07-71d4f0f06ee1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: degroff@netcom.com (21012d)\n",
            "Subject: Re: Venus Lander for Venus Conditions.\n",
            "Organization: Netcom Online Communications Services (408-241-9760 login: guest)\n",
            "Lines: 8\n",
            "\n",
            "\n",
            "  I doubt there are good prospects for  a self armoring system\n",
            "for venus surface conditions (several hundred degrees, very high\n",
            "pressure of CO2, possibly sulfuric and nitric acids or oxides\n",
            "but it is a notion to consider for outer planets rs where you might\n",
            "pick up ices under less extream upper atmosphere conditions buying\n",
            "deeper penetration.  A nice creative idea, unlikly but worthy of\n",
            "thinking about.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(newsgroups_train.data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "439ebe49",
      "metadata": {
        "id": "439ebe49",
        "outputId": "5a697293-02bc-4ad9-c2bb-2ab8d1175290",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism', 'comp.graphics', 'sci.space']"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "newsgroups_train['target_names']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9349ab53",
      "metadata": {
        "id": "9349ab53"
      },
      "source": [
        "The classnames we will pass to the classifier in training are integers.\n",
        "The integers are aligned with the class names in the order in target_names.\n",
        "Therefore we can set up a simple decoder dictionary that maps from class indices to class names:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "fd8f7772",
      "metadata": {
        "id": "fd8f7772",
        "outputId": "635365a2-7253-4608-dafb-250ff92a4817",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.str_('sci.space')"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "first_class = newsgroups_train.target[0]\n",
        "print(first_class)\n",
        "decoder = np.array(newsgroups_train.target_names)\n",
        "decoder[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8e08833",
      "metadata": {
        "id": "e8e08833"
      },
      "source": [
        "###  Training\n",
        "\n",
        "Train a logistic regression classifier on this multiclass problem.\n",
        "Also test it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "ed95c751",
      "metadata": {
        "id": "ed95c751",
        "outputId": "d1b596c0-2366-49de-fa0f-56ef06556736",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1657, 29663)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9401088929219601"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ],
      "source": [
        "# Not usually prize-winning with language data\n",
        "# vectorizer = CountVectorizer()\n",
        "\n",
        "##########  Mapping from a sequence of texts to a feature representation of the data\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectors_train = vectorizer.fit_transform(newsgroups_train.data)\n",
        "vectors_test = vectorizer.transform(newsgroups_test.data)\n",
        "# (1657, 29663)\n",
        "# 1,657 documents. 29663 features.  Why so many features?  That's how many\n",
        "# distinct vocab items cropped up in these 1,657 documents.  We use words\n",
        "# as features in our language model as well but there are way fewer\n",
        "# features because our training data consists of filtered document texts and therefore a filtered vocab.\n",
        "print(vectors_train.shape)\n",
        "########## End of feature  mapping #####################################\n",
        "\n",
        "clf = LogisticRegression(solver=\"saga\")\n",
        "# targets are [0,1,2]  aligned with newsgroups_train.target_names\n",
        "clf.fit(vectors_train, newsgroups_train.target)\n",
        "#  The usual thing we do with trained classifiers\n",
        "pred = clf.predict(vectors_test)\n",
        "metrics.f1_score(newsgroups_test.target, pred, average=\"micro\")\n",
        "#  f1 score average = \"micro\"  0.9407548825982005"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39f663bd",
      "metadata": {
        "id": "39f663bd"
      },
      "source": [
        "##  Predicting probabilities\n",
        "\n",
        "For this assignment we're more interested in having the clasifier produce probabilities:\n",
        "\n",
        "We classify a fresh example using `predict_proba`.\n",
        "\n",
        "Notice there are three probabilities.  That's because there are three classes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "0db54e89",
      "metadata": {
        "id": "0db54e89",
        "outputId": "42b3baae-3073-48ee-9837-ff3955659c1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.05980826, 0.18001922, 0.76017252]])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ],
      "source": [
        "space_text = [\"Space is the final frontier.\"]\n",
        "probs = clf.predict_proba(vectorizer.transform(space_text))\n",
        "probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "13cb803f",
      "metadata": {
        "id": "13cb803f",
        "outputId": "0e2753a6-edc6-42a9-c0b6-d7d23d7101a7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.9999999999999999)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "probs.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60dfc594",
      "metadata": {
        "id": "60dfc594"
      },
      "source": [
        "Class with the highest prob:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "b4de3dc5",
      "metadata": {
        "id": "b4de3dc5",
        "outputId": "55e0c561-88c7-4cf2-b6ac-8217335ff8a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(2)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "probs.argmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "9ab59117",
      "metadata": {
        "id": "9ab59117",
        "outputId": "27282e62-49ff-4b15-a256-ed0e0db6bcc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.str_('sci.space')"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "decoder = np.array(newsgroups_train.target_names)\n",
        "decoder[probs.argmax()]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bcce0ee",
      "metadata": {
        "id": "1bcce0ee"
      },
      "source": [
        "Which is the same answer I could have gotten through `predict`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "ec560a8d",
      "metadata": {
        "id": "ec560a8d",
        "outputId": "79b10f5d-6cc1-486a-dfa4-5e79f21827e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.str_('sci.space')"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "decoder[clf.predict(vectorizer.transform(space_text))[0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9950bde",
      "metadata": {
        "id": "a9950bde"
      },
      "source": [
        "Note  that in our language modeling example you didn't need to call `vectorizer.transform`.\n",
        "I wrote `prepare_korpus` to do that job, because I wanted some custom \"vectorizing\".\n",
        "But the output was still a 2D array with the same number of rows as there were histories\n",
        "to classify and the same number of columns as there were features to classify with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "b07c8bfe",
      "metadata": {
        "id": "b07c8bfe",
        "outputId": "ffd4ffb1-31ed-4b7d-a9e1-23edc51bd3d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probs shape:  (2, 3)\n",
            "Predicted class indices:  [2 1]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['sci.space', 'comp.graphics'], dtype='<U13')"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "# For a sequence of n inputs predict_proba will produce a  nx3 array.  Here n=2\n",
        "texts = [\"Space is the final frontier.\", \"Rasters are common in computer images.\"]\n",
        "probs = clf.predict_proba(vectorizer.transform(texts))\n",
        "print(\"Probs shape: \", probs.shape)\n",
        "cls_names = newsgroups_train.target_names\n",
        "cls_idxs = probs.argmax(axis=1)\n",
        "print(\"Predicted class indices: \",cls_idxs)\n",
        "decoder = np.array(cls_names)\n",
        "decoder[cls_idxs]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9701581d",
      "metadata": {
        "id": "9701581d"
      },
      "source": [
        "##  Retrieving the predicted probabilities for a sequence of classes\n",
        "\n",
        "Hint:  This discussion relates to computing the perplexity of the data:\n",
        "\n",
        "Now suppose in the interest of finding the **hard** examples in the test set,  I want to know the probability\n",
        "my trained classifier assigns to the **correct** class for each example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "id": "f3038393",
      "metadata": {
        "id": "f3038393"
      },
      "outputs": [],
      "source": [
        "pred_probs = clf.predict_proba(vectors_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "edb0dc6a",
      "metadata": {
        "id": "edb0dc6a",
        "outputId": "978707c7-8cbe-46d2-c2cb-42c5509cf875",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1102, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "pred_probs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5721ed2e",
      "metadata": {
        "id": "5721ed2e"
      },
      "source": [
        "I need to retrieve a different column index from each row of `pred_probs`, as dictated by the correct classes\n",
        "for the test data (`newsgroups_test.target`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "id": "55fa9d9c",
      "metadata": {
        "id": "55fa9d9c",
        "outputId": "11ad6678-2159-45a5-9619-2573a4a8ec22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([2, 1, 1, ..., 0, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": [
        "newsgroups_test.target"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d1f49fa",
      "metadata": {
        "id": "7d1f49fa"
      },
      "source": [
        "This can be done via **fancy indexing** of the probs array. For a 1D array we pass a list containing\n",
        "a sequence of the indices we want."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "37578beb",
      "metadata": {
        "scrolled": true,
        "id": "37578beb",
        "outputId": "c15e84d8-0c46-47d7-8012-e76a8140d80b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 4  7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61]\n",
            "(20,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10, 28, 37, 49])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "a = np.arange(4,62,3)\n",
        "print(a)\n",
        "print(a.shape)\n",
        "a[[2,8,11,15]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "288700c4",
      "metadata": {
        "id": "288700c4"
      },
      "source": [
        "For a 2D array, we pass two sequences, one for the row indices we want, the other for the column indices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "3a894862",
      "metadata": {
        "id": "3a894862",
        "outputId": "f8a6d8e6-752e-45c9-9e54-0f6929f809b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 4  7 10 13]\n",
            " [16 19 22 25]\n",
            " [28 31 34 37]\n",
            " [40 43 46 49]\n",
            " [52 55 58 61]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22, 37, 55])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "aa = a.reshape((5,4))\n",
        "print(aa)\n",
        "# The second element retrieved is aa[2,3]\n",
        "aa[[1,2,4],[2,3,1]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1bf0354",
      "metadata": {
        "id": "f1bf0354"
      },
      "source": [
        "Back to our original problem.  We want an array consisting of one element from each row,\n",
        "the element corresponding to the correct class for that row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "bfcc48ea",
      "metadata": {
        "id": "bfcc48ea",
        "outputId": "6eccd462-0fe9-4874-b8e8-93f8a61ac459",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.8863373 , 0.91611412, 0.6824465 , ..., 0.90234175, 0.68914973,\n",
              "       0.81725171])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "num_rows = len(newsgroups_test.target)\n",
        "all_rows_idxs = list(range(num_rows))\n",
        "column_idxs = list(newsgroups_test.target)\n",
        "\n",
        "prediction_probs_for_correct_classes = pred_probs[all_rows_idxs,column_idxs]\n",
        "prediction_probs_for_correct_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be90526b",
      "metadata": {
        "id": "be90526b"
      },
      "source": [
        "To find the lowest prob assigned to a correct class on the test set we first find its index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "3af09986",
      "metadata": {
        "id": "3af09986",
        "outputId": "304c188d-1274-457a-ff21-09848011b720",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.int64(548)"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "example_idx = prediction_probs_for_correct_classes.argmin()\n",
        "example_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6f16018",
      "metadata": {
        "id": "b6f16018"
      },
      "source": [
        "Here's the probability assigned to the correct class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "a2259f3c",
      "metadata": {
        "id": "a2259f3c",
        "outputId": "97492075-bf5f-47dc-b48c-1eecde5effef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.14183114567444172)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "prediction_probs_for_correct_classes[example_idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66c18ffe",
      "metadata": {
        "id": "66c18ffe"
      },
      "source": [
        "And here is that example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "7c7fe9ae",
      "metadata": {
        "id": "7c7fe9ae",
        "outputId": "d22d2907-a2ea-4fbe-8352-2451084e83b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: gkm@wampyr.cc.uow.edu.au (Glen K Moore)\n",
            "Subject: Fax/email wanted for Louis Friedman/Planetary Society\n",
            "Organization: University of Wollongong, NSW, Australia.\n",
            "Lines: 7\n",
            "NNTP-Posting-Host: wampyr.cc.uow.edu.au\n",
            "Summary: Want to obtain fax/email address for Planetary Society\n",
            "Keywords: Planetary Friedman\n",
            "\n",
            "If available please send to\n",
            "Glen Moore\n",
            "Director\n",
            "Science Centre\n",
            "Wollongong, Australia\n",
            "fax: 61 42 213151   email: gkm@cc.uow.edu.au\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "doc548 = newsgroups_test.data[548]\n",
        "print(doc548)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f44a865",
      "metadata": {
        "id": "4f44a865"
      },
      "source": [
        "Confirming the probs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "5479c0e4",
      "metadata": {
        "scrolled": true,
        "id": "5479c0e4",
        "outputId": "2570ea6a-508c-4892-dab6-a862790f83f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.06889007, 0.78927879, 0.14183115]])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "probs = clf.predict_proba(vectorizer.transform([doc548]))\n",
        "probs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b1d76ee",
      "metadata": {
        "id": "7b1d76ee"
      },
      "source": [
        "##  Precision by class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "b9f1be4e",
      "metadata": {
        "id": "b9f1be4e"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95c9d3d1",
      "metadata": {
        "id": "95c9d3d1"
      },
      "source": [
        "Here `clf` is the classifier trained above. Note the use of `average=None`.  This gets the class\n",
        "by class precision results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "38f6c8fc",
      "metadata": {
        "id": "38f6c8fc",
        "outputId": "ec08b2fa-a2b0-4aba-a5f3-84160bcf00d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.97674419, 0.90510949, 0.94871795])"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "pred = clf.predict(vectors_test)\n",
        "metrics.precision_score(newsgroups_test.target, pred, average=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d166e048",
      "metadata": {
        "id": "d166e048"
      },
      "source": [
        "Note the order of the arguments matters for precision.  Swapping predicted and true labelings changes the scores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "ba3e1199",
      "metadata": {
        "id": "ba3e1199",
        "outputId": "47df0589-a7e1-4bc7-91be-b840863472af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.92163009, 0.9562982 , 0.93908629])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "metrics.precision_score(pred,newsgroups_test.target, average=None)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7b60c64",
      "metadata": {
        "id": "d7b60c64"
      },
      "source": [
        "The first order given is correct, as the function signature shows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "fe55e808",
      "metadata": {
        "id": "fe55e808",
        "outputId": "bd940425-73ba-42ec-856d-b96172eb7c6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Signature (y_true, y_pred, *, labels=None, pos_label=1, average='binary', sample_weight=None, zero_division='warn')>"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "signature(metrics.f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5086ed9f",
      "metadata": {
        "id": "5086ed9f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}